[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimación Bayesiana",
    "section": "",
    "text": "Materia electiva de la Licenciatura en Ciencia de Datos.\n\n\n\n\n\n\n  \n  \n  \n    Docente:\nGuillermo Solovey, Instituto de Cálculo.\n    Clases:\nDesde el Lunes 20 de Marzo al Lunes 3 de Julio.\n    Lunes:\nde 10 a 12hs en el aula 1115 (cero+infinito).\n    Martes:\nde 10 a 12hs en el laboratorio 1109 (cero+infinito).\n    Correlativa:\nIntroducción a la Estadística y la Ciencia de Datos.\n  \n  \n  \n\n\n\n\n\n\n Instituto de Cálculo\n\n\n Facultad de Ciencias Exactas y Naturales, UBA"
  },
  {
    "objectID": "cronograma.html",
    "href": "cronograma.html",
    "title": "Cronograma",
    "section": "",
    "text": "Nota: Este cronograma es muy preliminar. Se va a actualizar en Marzo.\n\n\n\n\n\n\n\n\n\n  \n  \n    \n      semana\n      fecha\n      tema\n      lectura\n      slides\n      notas\n      tp\n    \n  \n  \n    1\n\nIntroducción a estadística bayesiana\nBayes Rules! c.1-2\n\n\n\n    1\n\nLabo 01\n\n\n\n\n    2\n\nEl modelo Beta-Binomial\nBayes Rules! c.3\n\n\n\n    2\n\nTP1: Regla de Bayes y ejercicios de simulación\n\n\n\n\n    3\n\nBalance y secuencialidad. Familias conjugadas. Gamma-Poisson, Normal-Normal\nBayes Rules! c.4-5\n\n\n\n    3\n\nTP2: Beta-Binomial\n\n\n\n\n    4\n\nSimulación de la Posterior\nBayes Rules! c.6-7\n\n\n\n    4\n\nTP4\n\n\n\n\n    5\n\nInferencia Posterior y Predicción\nBayes Rules! c.8\n\n\n\n    5\n\nTP4\n\n\n\n\n    6\n\n\n\n\n\n\n    6\n\n\n\n\n\n\n    7\n\nRegresión lineal simple. Regularización LASSO y RIDGE.\nBayes Rules! c. 9, 10\n\n\n\n    7\n\nTP5\n\n\n\n\n    8\n\nRegresión lineal. Variables Categóricas. Evaluación de la calidad del ajuste.\nBayes Rules! c. 11.\n\n\n\n    8\n\nTP6\n\n\n\n\n    9\n\nRegresión de Poisson y Binomial Negativa\nBayes Rules! c.12\n\n\n\n    9\n\nTP7\n\n\n\n\n    10\n\nRegresión logística. Naive Bayes\nBayes Rules! c.13\n\n\n\n    10\n\nTP8\n\n\n\n\n    11\n\n\n\n\n\n\n    11\n\n\n\n\n\n\n    12\n\nModelos jerárquicos sin predictores\nBayes Rules! c.15-16\n\n\n\n    12\n\nTP9\n\n\n\n\n    13\n\nModelos jerárquicos con predictores\nBayes Rules! c.17\n\n\n\n    13\n\nTP10\n\n\n\n\n    14\n\nModelos jerárquicos de presicción y clasificación (no-normales)\nBayes Rules! c.18\n\n\n\n    14\n\nTP11\n\n\n\n\n    15\n\n\n\n\n\n\n    15\n\n\n\n\n\n\n    16\n\nPresentación final\n\n\n\n\n    16\n\nPresentación final"
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Estimación Bayesiana",
    "section": "Course materials",
    "text": "Course materials\n\nBooks\nWe’ll be working through two textbooks throughout the semester:\n\nRichard McElreath, Statistical Rethinking: A Bayesian Course with Examples in R and Stan\n\n\n\n\nAlicia A. Johnson, Miles Q. Ott, and Mine Dogucu, Bayes Rules! An Introduction to Applied Bayesian Modeling\n\n\n\nBayes Rules! is available online for free. The book for Statistical Rethinking is not free ($70 on Amazon), but Richard McElreath has provided 20 video lectures with accompanying slides and homework assignments and answer keys.\nWe’ll read all of Bayes Rules!, all of Statistical Rethinking, watch all of McElreath’s Statistical Rethinking lectures, and complete a bunch of the assignments and homework questions from both books.\n\nDocente\n\nDocente: Dr. Guillermo Solovey\nOficina: 2605, Edificio Cero+Infinito, Exactas-UBA\n\n\n\nFechas\n\nInscripción: Primer cuatrimestre 2023. Inicio de clases: semana del 20 de Marzo.\nDictado: Primer cuatrimestre 2023. Inicio de clases: semana del 20 de Marzo.\nHorarios: 4 horas semanales en días y horario a definir.\n\n\n\nCorrelativas\n\nIntroducción a la Estadística y la Ciencia de Datos.\n\n\n\nCódigo de inscripción en el SIU\n\nGrado: a definir\nDoctorado: a definir\n\n\n\nBibliografía\n\nVamos a usar principalmente Bayes Rules! y Statistical Rethinking.\n\n\n\n Instituto de Cálculo\n\n\n Facultad de Ciencias Exactas y Naturales, UBA"
  },
  {
    "objectID": "programa.html",
    "href": "programa.html",
    "title": "Programa",
    "section": "",
    "text": "Distinguir entre el enfoque estadístico frecuentista y bayesiano.\nConstruir modelos bayesianos para situaciones simples.\nUsar fluidamente R y Stan para implementar modelos estadísticos bayesianos.\nHacer predicciones e interpretar los resultados de un modelo bayesiano."
  },
  {
    "objectID": "programa.html#bibliografía",
    "href": "programa.html#bibliografía",
    "title": "Programa",
    "section": "Bibliografía",
    "text": "Bibliografía\nprincipal:\n\nAlicia A. Johnson, Miles Q. Ott, and Mine Dogucu, Bayes Rules! An Introduction to Applied Bayesian Modeling\nRichard McElreath, Statistical Rethinking: A Bayesian Course with Examples in R and Stan, 2nd edition.\n\n\ncomplementaria:\n\nBen Lambert, A Student’s Guide to Bayesian Statistics.\nOsvaldo A. Martin, Ravin Kumar y Junpeng Lao, Bayesian Modeling and Computation in Python.\nPeter D. Hoff, A first course in Bayesian statistical methods.\nCameron Davidson-Pilon, Bayesian methods for hackers: probabilistic programming and Bayesian inference.\nDavid Robinson, Introduction to empirical bayes: examples from baseball statistics.\nAndrew Gelman, Jennifer Hill, and Aki Vehtari, Regression and other stories."
  },
  {
    "objectID": "programa.html#programa",
    "href": "programa.html#programa",
    "title": "Programa",
    "section": "Programa",
    "text": "Programa\n\nUnidad 1\nFundamentos bayesianos: Aprender cómo pensar bayesianamente y cómo crear modelos bayesianos básicos.\nIntroducción a la estadística bayesiana. Diferencias entre estadística bayesiana y frecuentista. Pensar bayesianamente. Modelo beta-binomial. Equilibrio entre el prior y los datos. Análisis bayesiano secuencial. Familias conjugadas.\n\n\nUnidad 2\nSimulación y análisis de la distribución posterior: Herramientas computacionales para simular la distribución posterior en modelos bayesianos complejos.\nAnalizar modelos simulados y exactos para hacer inferencia y sacar conclusiones. Aproximar la distribución posterior. Método de grilla, Metrópolis-Hastings y Markov Chain Monte Carlo (MCMC). Implementación y diagnóstico en R. Estimación de parámetros Testeo de hipótesis. Predicción.\n\n\nUnidad 3\nModelos bayesianos de regresión y clasificación: Extender los modelos bayesianos a casos en los que la variable respuesta es contínua (regresión) y categórica (clasificación).\nRegresión Normal. Regresión múltiple. Variables de control y confusoras. Evaluación, diagnóstico y comparación de modelos de regresión. Regresión de Poisson. Naive-Bayes. Regresión logística.\n\n\nUnidad 4\nModelos bayesianos jerárquicos: Modelos bayesianos para datos multi-nivel, como datos longitudinales y de medidas repetidas.\nModelo complete-pool y no-pool. Modelos de pooling parcial. Modelo jerárquico normal sin predictores. Modelos de regresión y clasificación jerárquicos."
  },
  {
    "objectID": "index.html#docente",
    "href": "index.html#docente",
    "title": "Estimación Bayesiana",
    "section": "Docente",
    "text": "Docente\n\nDocente: Dr. Guillermo Solovey\nOficina: 2605, Edificio Cero+Infinito, Exactas-UBA"
  },
  {
    "objectID": "index.html#fechas",
    "href": "index.html#fechas",
    "title": "Estimación Bayesiana",
    "section": "Fechas",
    "text": "Fechas\n\nInscripción: desde el 22 de Febrero al 5 de Marzo de 2023.\nDictado: Primer cuatrimestre 2023. Inicio de clases: semana del 20 de Marzo.\nHorarios: 4 horas semanales en días y horario a definir."
  },
  {
    "objectID": "index.html#correlativas",
    "href": "index.html#correlativas",
    "title": "Estimación Bayesiana",
    "section": "Correlativas",
    "text": "Correlativas\n\nIntroducción a la Estadística y la Ciencia de Datos.\n\n\n\n Instituto de Cálculo\n\n\n Facultad de Ciencias Exactas y Naturales, UBA"
  },
  {
    "objectID": "index.html#código-de-inscripción-en-el-siu",
    "href": "index.html#código-de-inscripción-en-el-siu",
    "title": "Estimación Bayesiana",
    "section": "Código de inscripción en el SIU",
    "text": "Código de inscripción en el SIU\n\nGrado: a definir\nDoctorado: a definir"
  },
  {
    "objectID": "index.html#bibliografía",
    "href": "index.html#bibliografía",
    "title": "Estimación Bayesiana",
    "section": "Bibliografía",
    "text": "Bibliografía\n\n?var:course.text\n\n\n\n Instituto de Cálculo\n\n\n Facultad de Ciencias Exactas y Naturales, UBA"
  },
  {
    "objectID": "eb2023.html",
    "href": "eb2023.html",
    "title": "Estimación bayesiana",
    "section": "",
    "text": "Una máquina funciona perfectamente mientras tiene una sustancia que la protege. Sin embargo, esta sustancia se va consumiendo y cuando se agota, después de un tiempo \\(\\theta\\), puede fallar en algún momento aleatorio que sigue una distribución exponencial. El tiempo que pasa hasta que produce la falla,\\(x\\), sigue una distribución exponencial truncada dada por:\n\\[\nf(x|\\theta) = \\left\\{\n\\begin{array}{ll}\n      0 & x < \\theta \\\\\n      e^{-(x-\\theta)} & x > \\theta \\\\\n\\end{array}\n\\right.\n\\]\nSe mide el tiempo de falla de tres máquinas obteniendo: \\(\\text{datos} = \\{10,12,15\\}\\). El objetivo es, a partir de estos datos, inferir \\(\\theta\\). En particular, queremos un intervalo de confianza frecuentista y un intervalo de credibilidad bayesiano para \\(\\theta\\)."
  },
  {
    "objectID": "eb2023.html#intervalo-frecuentista",
    "href": "eb2023.html#intervalo-frecuentista",
    "title": "Estimación bayesiana",
    "section": "Intervalo frecuentista",
    "text": "Intervalo frecuentista\nEl intervalo de confianza asintótico para un estimador \\(\\theta\\) es \\(\\text{CI}_{95} \\approx \\hat{\\theta} \\pm 2~\\sqrt{\\mathbb{V}(\\hat{\\theta})}\\). Observando que \\(y = x - \\theta\\) es exponencial con parámetro 1, la esperanza de \\(y\\) es \\(E(y)=1\\) y la varianza \\(\\mathbb{V}(y)=1\\), para \\(x\\) tenemos:\n\\[\n\\begin{array}{l}\nE(x) = \\theta + 1 \\\\\n\\mathbb{V}(x) = \\mathbb{V}(y+\\theta) = 1\n\\end{array}\n\\]\nPor lo tanto, un estimador de \\(\\theta\\) es:\n\\[\n\\hat{\\theta} = \\frac{1}{N} \\sum_{i=1}^N x_i - 1 \\\\\n\\] que tiene una varianza \\(\\mathbb{V}(\\hat{\\theta}) = \\frac{1}{N^2} \\sum \\mathbb{V}(x_i) = \\frac{1}{N}\\). Por lo tanto, el intervalo de confianza del 95% asintótico es:\n\\[\n\\text{CI}_{95} \\approx (\\hat{\\theta} - 2 / \\sqrt{N}, \\hat{\\theta} + 2 / \\sqrt{N}) \\\\\n\\]\nUsando los \\(\\text{datos} = \\{10,12,15\\}\\), obtenemos que:\n\\[\n\\begin{array}{ll}\n\\color{blue}{ \\hat{\\theta}   } & \\color{blue}{ \\approx 11.3 } \\\\\n\\color{blue}{ \\text{CI}_{95} } & \\color{blue}{ \\approx (10.2, 12.5) }\n\\end{array}\n\\]\nEste intervalo de confianza llama la atención porque claramente \\(\\theta\\), el tiempo a partir del cual la máquina comienza a fallar no puede ser mayor que el mínimo entre todos los tiempos de falla que se midieron. O sea: \\(\\theta <10\\)!!"
  },
  {
    "objectID": "eb2023.html#intervalo-bayesiano",
    "href": "eb2023.html#intervalo-bayesiano",
    "title": "Estimación bayesiana",
    "section": "Intervalo bayesiano",
    "text": "Intervalo bayesiano\nTenemos que encontrar la distribución de probabilidad posterior \\(P(\\theta|\\text{datos})\\):\n\\[\n\\require{mathtools}\n\\definecolor{bayesred}{RGB}{147, 30, 24}\n\\definecolor{bayesblue}{RGB}{32, 35, 91}\n\\definecolor{bayesorange}{RGB}{218, 120, 1}\n\\definecolor{grey}{RGB}{128, 128, 128}\n\\color{bayesorange} \\overbracket[0.25pt]{P(\\theta \\mid \\text{datos})}^{\\text{Posterior}} \\sim\n\\color{bayesred}    \\overbracket[0.25pt]{P(\\theta)}^{\\text{Prior}} \\times\n\\color{bayesblue}   \\overbracket[0.25pt]{P(\\text{datos} \\mid \\theta)}^{\\text{Likelihood}}\n\\] Como prior vamos a usar una distribución uniforme entre 0 y algún número elevado \\(\\theta_{max}\\) que sea superior a cualquier tiempo de falla razonable (por ejemplo \\(\\theta_{max}=10^{10}\\)). El likelihood es:\n\\[\n\\begin{array}{ll}\nL(\\theta) & = P(\\text{datos} \\mid \\theta) = \\prod_{i=1}^{N} f(x_i \\mid \\theta) \\\\\n& \\sim \\left\\{ \\begin{array}{ll}\n      \\exp(N\\theta) & \\theta < \\text{min}(x_i) \\\\\n      0             & \\theta > \\text{min}(x_i)\n\\end{array}\n\\right.\n\\end{array}\n\\]\nPor lo tanto, la distribución posterior es:\n\\[\n\\definecolor{bayesorange}{RGB}{218, 120, 1}\n\\color{bayesorange}{P(\\theta \\mid \\text{datos})}\n\\sim \\left\\{ \\begin{array}{ll}\n      e^{3\\theta} & 0 < \\theta < 10 \\\\\n      0             & \\theta > 10\n\\end{array}\n\\right.\n\\] Ahora, un intervalo del 95% de credibilidad para \\(\\theta\\) es un intervalo \\((a,10)\\) tal que\n\\[\n\\frac{\\int_a^{10} e^{3\\theta} ~  d\\theta }{\\int_0^{10} e^{3\\theta} ~  d\\theta } = 0.95\n\\]\nHaciendo la cuenta se obtiene que \\(a=9\\) y por lo tanto, condicional al modelo y a los datos, podemos afirmar que existe una probabilidad del 95% de que el tiempo en el cual se agota el líquido necesario para que funcione la máquina, \\(\\theta\\), esté entre 9 y 10.\n\n\n\n\n\n\n\n\n\n\n\nRecordar\n\n\n\nUn intervalo de confianza no es una afirmación probabilística sobre \\(\\theta\\)."
  },
  {
    "objectID": "eb2023.html#funciona-mal-el-intervalo-frecuentista",
    "href": "eb2023.html#funciona-mal-el-intervalo-frecuentista",
    "title": "Estimación bayesiana",
    "section": "¿Funciona mal el intervalo frecuentista?",
    "text": "¿Funciona mal el intervalo frecuentista?\n¿Por qué el intervalo frecuentista queda enteramente por afuera de la región aceptable para el parámetro \\(\\theta\\)? El intervalo de confianza frecuentista, por definición, contiene al parámetro verdadero con una probabilidad del 95%. Esto quiere decir que si tomamos muchas muestras de tamaño 3 (como en este problema) y para cada muestra aleatoria calculamos el intervalo de confianza, el 95% van a incluir al verdadero. Lo que ocurre con estos datos es que por mala suerte nos tocó uno de los intervalos de ese 5% que no contiene al verdadero.\nVerifiquemos que el intervalo de confianza asintótico funciona bien. Al ser asintótico, debería ser correcto para un tamaño de muestra grande, pero en este caso \\(n=3\\) y podría no ser válido como intervalo de confianza. Para ver si es el caso, vamos a simular muchas muestras aleatorias de \\(n=3\\) para un parámetro \\(\\theta=10\\) fijo y para cada una vamos a verificar si el intervalo de confianza contiene al valor verdadero.\n\nn     = 3\ntheta = 10\n\nNit = 10000\nok  = 0\nfor (i in 1:Nit){\n\n  x     = rexp(n, 1) + theta\n  theta.e = mean(x) - 1\n  ci      = c(theta.e - 2/sqrt(n), theta.e + 2/sqrt(n))\n  \n  if (theta > ci[1] & theta < ci[2]){\n    ok = ok + 1\n  }\n}\n\n# cobertura del intervalo de confianza\nprint(ok/Nit)\n\n[1] 0.954\n\n\nDe las 10.000 muestras aleatorias con \\(N=3\\) simuladas alrededor del 95% contiene al parámetro \\(\\theta=10\\)."
  },
  {
    "objectID": "eb2023.html#problema",
    "href": "eb2023.html#problema",
    "title": "Estimación bayesiana",
    "section": "Problema",
    "text": "Problema\nOriginalmente planteado por Berger y Wolpert (1984), yo lo tomé del libro de Wasserman “All of Statistics” (¡excelente!).\nSupongamos que hay dos variables aleatorias \\(X_1\\) y \\(X_2\\) independientes que pueden tomar el valor 1 o -1 con igual probabilidad (¡dos monedas!). Definimos otras dos variables así:\n\\[\n\\begin{array}{ll}\nY_1 &= \\theta + X_1 \\\\\nY_2 &= \\theta + X_2\n\\end{array}\n\\] Supongamos que sólo se miden \\(Y_1\\) e \\(Y_2\\) y se quiere estimar \\(\\theta\\) que está fijo.\n1- Verificar que \\(C\\), definido a continuación, es un intervalo de confianza del 75% para \\(\\theta\\):\n\\[\nC = \\left\\{\n\\begin{array}{ll}\n      Y_1 - 1        & \\text{si } Y_1 = Y_2 \\\\\n      (Y_1 + Y_2)/2  & \\text{si } Y_1 \\ne Y_2\n\\end{array}\n\\right.\n\\]\n2- Supongan ahora que se mide en un experimento \\(y_1 = 15\\) y \\(y_2=17\\), ¿cuál es el intervalo de confianza del 75% para \\(\\theta\\)? ¿cuánto vale \\(\\theta\\)?"
  },
  {
    "objectID": "intervalos.html",
    "href": "intervalos.html",
    "title": "Estimación bayesiana",
    "section": "",
    "text": "Una máquina funciona perfectamente mientras tiene una sustancia que la protege. Sin embargo, esta sustancia se va consumiendo y cuando se agota, después de un tiempo \\(\\theta\\), puede fallar en algún momento aleatorio que sigue una distribución exponencial. El tiempo que pasa hasta que produce la falla,\\(x\\), sigue una distribución exponencial truncada dada por:\n\\[\nf(x|\\theta) = \\left\\{\n\\begin{array}{ll}\n      0 & x < \\theta \\\\\n      e^{-(x-\\theta)} & x > \\theta \\\\\n\\end{array}\n\\right.\n\\]\nSe mide el tiempo de falla de tres máquinas obteniendo: \\(\\text{datos} = \\{10,12,15\\}\\). El objetivo es, a partir de estos datos, inferir \\(\\theta\\). En particular, queremos un intervalo de confianza frecuentista y un intervalo de credibilidad bayesiano para \\(\\theta\\)."
  },
  {
    "objectID": "intervalos.html#intervalo-frecuentista",
    "href": "intervalos.html#intervalo-frecuentista",
    "title": "Estimación bayesiana",
    "section": "Intervalo frecuentista",
    "text": "Intervalo frecuentista\nEl intervalo de confianza asintótico para un estimador \\(\\theta\\) es \\(\\text{CI}_{95} \\approx \\hat{\\theta} \\pm 2~\\sqrt{\\mathbb{V}(\\hat{\\theta})}\\). Observando que \\(y = x - \\theta\\) es exponencial con parámetro 1, la esperanza de \\(y\\) es \\(E(y)=1\\) y la varianza \\(\\mathbb{V}(y)=1\\), para \\(x\\) tenemos:\n\\[\n\\begin{array}{l}\nE(x) = \\theta + 1 \\\\\n\\mathbb{V}(x) = \\mathbb{V}(y+\\theta) = 1\n\\end{array}\n\\]\nPor lo tanto, un estimador de \\(\\theta\\) es:\n\\[\n\\hat{\\theta} = \\frac{1}{N} \\sum_{i=1}^N x_i - 1 \\\\\n\\] que tiene una varianza \\(\\mathbb{V}(\\hat{\\theta}) = \\frac{1}{N^2} \\sum \\mathbb{V}(x_i) = \\frac{1}{N}\\). Por lo tanto, el intervalo de confianza del 95% asintótico es:\n\\[\n\\text{CI}_{95} \\approx (\\hat{\\theta} - 2 / \\sqrt{N}, \\hat{\\theta} + 2 / \\sqrt{N}) \\\\\n\\]\nUsando los \\(\\text{datos} = \\{10,12,15\\}\\), obtenemos que:\n\\[\n\\begin{array}{ll}\n\\color{blue}{ \\hat{\\theta}   } & \\color{blue}{ \\approx 11.3 } \\\\\n\\color{blue}{ \\text{CI}_{95} } & \\color{blue}{ \\approx (10.2, 12.5) }\n\\end{array}\n\\]\nEste intervalo de confianza llama la atención porque claramente \\(\\theta\\), el tiempo a partir del cual la máquina comienza a fallar no puede ser mayor que el mínimo entre todos los tiempos de falla que se midieron. O sea: \\(\\theta <10\\)!!"
  },
  {
    "objectID": "intervalos.html#intervalo-bayesiano",
    "href": "intervalos.html#intervalo-bayesiano",
    "title": "Estimación bayesiana",
    "section": "Intervalo bayesiano",
    "text": "Intervalo bayesiano\nTenemos que encontrar la distribución de probabilidad posterior \\(P(\\theta|\\text{datos})\\):\n\\[\n\\require{mathtools}\n\\definecolor{bayesred}{RGB}{147, 30, 24}\n\\definecolor{bayesblue}{RGB}{32, 35, 91}\n\\definecolor{bayesorange}{RGB}{218, 120, 1}\n\\definecolor{grey}{RGB}{128, 128, 128}\n\\color{bayesorange} \\overbracket[0.25pt]{P(\\theta \\mid \\text{datos})}^{\\text{Posterior}} \\sim\n\\color{bayesred}    \\overbracket[0.25pt]{P(\\theta)}^{\\text{Prior}} \\times\n\\color{bayesblue}   \\overbracket[0.25pt]{P(\\text{datos} \\mid \\theta)}^{\\text{Likelihood}}\n\\] Como prior vamos a usar una distribución uniforme entre 0 y algún número elevado \\(\\theta_{max}\\) que sea superior a cualquier tiempo de falla razonable (por ejemplo \\(\\theta_{max}=10^{10}\\)). El likelihood es:\n\\[\n\\begin{array}{ll}\nL(\\theta) & = P(\\text{datos} \\mid \\theta) = \\prod_{i=1}^{N} f(x_i \\mid \\theta) \\\\\n& \\sim \\left\\{ \\begin{array}{ll}\n      \\exp(N\\theta) & \\theta < \\text{min}(x_i) \\\\\n      0             & \\theta > \\text{min}(x_i)\n\\end{array}\n\\right.\n\\end{array}\n\\]\nPor lo tanto, la distribución posterior es:\n\\[\n\\definecolor{bayesorange}{RGB}{218, 120, 1}\n\\color{bayesorange}{P(\\theta \\mid \\text{datos})}\n\\sim \\left\\{ \\begin{array}{ll}\n      e^{3\\theta} & 0 < \\theta < 10 \\\\\n      0             & \\theta > 10\n\\end{array}\n\\right.\n\\] Ahora, un intervalo del 95% de credibilidad para \\(\\theta\\) es un intervalo \\((a,10)\\) tal que\n\\[\n\\frac{\\int_a^{10} e^{3\\theta} ~  d\\theta }{\\int_0^{10} e^{3\\theta} ~  d\\theta } = 0.95\n\\]\nHaciendo la cuenta se obtiene que \\(a=9\\) y por lo tanto, condicional al modelo y a los datos, podemos afirmar que existe una probabilidad del 95% de que el tiempo en el cual se agota el líquido necesario para que funcione la máquina, \\(\\theta\\), esté entre 9 y 10.\n\n\n\n\n\n\n\n\n\n\n\nRecordar\n\n\n\nUn intervalo de confianza no es una afirmación probabilística sobre \\(\\theta\\)."
  },
  {
    "objectID": "intervalos.html#funciona-mal-el-intervalo-frecuentista",
    "href": "intervalos.html#funciona-mal-el-intervalo-frecuentista",
    "title": "Estimación bayesiana",
    "section": "¿Funciona mal el intervalo frecuentista?",
    "text": "¿Funciona mal el intervalo frecuentista?\n¿Por qué el intervalo frecuentista queda enteramente por afuera de la región aceptable para el parámetro \\(\\theta\\)? El intervalo de confianza frecuentista, por definición, contiene al parámetro verdadero con una probabilidad del 95%. Esto quiere decir que si tomamos muchas muestras de tamaño 3 (como en este problema) y para cada muestra aleatoria calculamos el intervalo de confianza, el 95% van a incluir al verdadero. Lo que ocurre con estos datos es que por mala suerte nos tocó uno de los intervalos de ese 5% que no contiene al verdadero.\nVerifiquemos que el intervalo de confianza asintótico funciona bien. Al ser asintótico, debería ser correcto para un tamaño de muestra grande, pero en este caso \\(n=3\\) y podría no ser válido como intervalo de confianza. Para ver si es el caso, vamos a simular muchas muestras aleatorias de \\(n=3\\) para un parámetro \\(\\theta=10\\) fijo y para cada una vamos a verificar si el intervalo de confianza contiene al valor verdadero.\n\nn     = 3\ntheta = 10\n\nNit = 10000\nok  = 0\nfor (i in 1:Nit){\n\n  x     = rexp(n, 1) + theta\n  theta.e = mean(x) - 1\n  ci      = c(theta.e - 2/sqrt(n), theta.e + 2/sqrt(n))\n  \n  if (theta > ci[1] & theta < ci[2]){\n    ok = ok + 1\n  }\n}\n\n# cobertura del intervalo de confianza\nprint(ok/Nit)\n\n[1] 0.9574\n\n\nDe las 10.000 muestras aleatorias con \\(N=3\\) simuladas alrededor del 95% contiene al parámetro \\(\\theta=10\\)."
  },
  {
    "objectID": "intervalos.html#problema",
    "href": "intervalos.html#problema",
    "title": "Estimación bayesiana",
    "section": "Problema",
    "text": "Problema\nOriginalmente planteado por Berger y Wolpert (1984), yo lo tomé del libro de Wasserman “All of Statistics” (¡excelente!).\nSupongamos que hay dos variables aleatorias \\(X_1\\) y \\(X_2\\) independientes que pueden tomar el valor 1 o -1 con igual probabilidad (¡dos monedas!). Definimos otras dos variables así:\n\\[\n\\begin{array}{ll}\nY_1 &= \\theta + X_1 \\\\\nY_2 &= \\theta + X_2\n\\end{array}\n\\] Supongamos que sólo se miden \\(Y_1\\) e \\(Y_2\\) y se quiere estimar \\(\\theta\\) que está fijo.\n1- Verificar que \\(C\\), definido a continuación, es un intervalo de confianza del 75% para \\(\\theta\\):\n\\[\nC = \\left\\{\n\\begin{array}{ll}\n      Y_1 - 1        & \\text{si } Y_1 = Y_2 \\\\\n      (Y_1 + Y_2)/2  & \\text{si } Y_1 \\ne Y_2\n\\end{array}\n\\right.\n\\]\n2- Supongan ahora que se mide en un experimento \\(y_1 = 15\\) y \\(y_2=17\\), ¿cuál es el intervalo de confianza del 75% para \\(\\theta\\)? ¿cuánto vale \\(\\theta\\)?"
  },
  {
    "objectID": "ejercicios.html",
    "href": "ejercicios.html",
    "title": "Guías de problemas",
    "section": "",
    "text": "Recomiendo hacer, además de estos, los ejercicios de los capítulos 1 a 3 del libro Bayes Rules!.\n\nSupongamos que queremos averiguar cuál es la superficie de la Tierra cubierta por agua, \\(\\theta\\). Para eso, lanzamos al aire un globo terráqueo y al atraparlo, registramos si nuestro dedo índice de la mano derecha quedó marcando agua o tierra. De esta manera obtenemos una secuencia de muestras aleatorias de una variable \\(\\text{Bernoulli}(\\theta)\\) independientes.\n\nCon ese procedimiento se obtienen 4 “agua” (A) y 11 “tierra” (T). Usando un prior uniforme para \\(\\theta\\), obtener la distribución posterior para \\(\\theta\\).\nUsando la distribución posterior calculada, obtener la distribución “posterior predictive” para las siguiente 5 muestras aleatorias\nUsar la distribución “posterior predictive” para calcular la probabilidad de obtener 3 o más A en las siguiente 5 realizaciones.\n\nSiguiente el procedimiento del ejercicio anterior y partiendo de una distribución uniforme, graficar la distribución posterior luego de observar A. Luego, usando esa distribución posterior como prior, calcular y graficar la distribición posterior luego de observar T. Continuar este proceso para ver cómo se actualiza secuencialmente la distribución posterior cuando sucesivamente obtenemos (después de las dos primeras muestras aleatorias) {A, A, A, T, A, T, A}.\nRepetir el ejercicio 2 pero partir de un prior triangular, con máximo en \\(\\theta = 0.5\\). Para hacerlo, hacer una aproximación de grilla para calcular la posterior. Esto es:\n\nDefinir un vector de valores de \\(\\theta\\) en los cuales se quiere estimar la posterior.\nCalcular el prior para cada valor \\(\\theta\\) en esa grilla.\nCalcular la función de likelihood para valor del parámetro.\nCalcular la posterior no normalizada multiplicando el prior por el likelihood.\nNormalizar la posterior dividiendo por la suma de todos sus valores.\n\nCrear una función que genere \\(N\\) datos del proceso aleatorio descripto en el ejercicio 1 tomando como parámetros \\(N\\) y \\(\\theta\\). Crear otra función que calcule la distribución posterior partiendo de un prior uniforme, genere muestras de esta distribución y con esas muestras devuelva un intervalo de credibilidad del 50% \\(\\theta\\). Con esta función, estimar el número mínimo de muestras aleatorias que se necesitan para estimar \\(\\theta\\) con un intervalo de credibilidad de ancho menor a 0.1.\nSupongamos que estamos de espaldas a una mesa de pool separada en dos por una línea vertical. Juan y Alicia participan del siguiente juego. Tiran una pelota a la mesa que cae en un punto cualquiera, al azar. Si cae a la izquierda de la línea, Alicia gana 1 punto. Si cae a la derecha, Juan gana 1 punto. Gana el primero que llega a 6 puntos. Después de tirar 8 bolas, Alicia suma 5 puntos y Juan 3.\n\nCalcular la distribución posterior para la probabilidad \\(\\theta\\) de que Juan gane el juego.\nTomar muestras de la posterior y con ellas obtener la esperanza de \\(\\theta\\). Comparar con el resultado visto en clase.\nSimular este juego para obtener la probabilidad esperada de que gana Juan el juego, entendida como la cantidad de veces que gana Juan el juego condicional a que va perdiendo 5 a 3."
  }
]