---
title: "Parcial"
toc: false
view: source
editor: source
---

$$ \text{Beta: }f(\theta \mid a,b) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} ~ \theta^{~a-1} ~ (1-\theta)^{~b-1} ~ \text{, para } \theta \in (0,1) $$

$$ \text{Gamma: } f(\lambda \mid r,s) = \frac{r^s}{\Gamma(s)} ~ \lambda^{~s-1} e^{-r \lambda} ~ \text{, para } \lambda >0 $$

$$ \text{Binomial: } f(y \mid N,\theta) = {N \choose y}   \theta^{~y} ~ (1-\theta)^{~N-y}  ~ \text{, para } y=0,1,2,...,N $$

$$ \text{Poisson: } f(y \mid \lambda) = \frac{\lambda^y~e^{-\lambda}}{y!} ~ \text{, para } y=0,1,2,... $$

1. Considerar un modelo beta-binomial con datos y = (y_1, ..., y_n), donde y_i = 1 si la variable i es un éxito y 0 en si es un fracaso. Asumir que la distribución a priori para la probabilidad de éxito theta es una beta(a,b). 

- Encontrar la distribución posterior para $\theta$.
- Suponer un dataset con $n=10$ observaciones, 3 de las cuáles son éxitosy asumir un prior beta(2,2). Graficar la distribución posterior para \theta y calcular la probabilidad de que $theta$ sea menor que 0.4.

1. Suponer una muestra y = (y_1, ..., y_n) donde y_i ~ Poisson(theta) para i=1,...,n. Asumir un prior gamma para theta con parámetro shape a y parámetros rate b.

- Usar la aproximación de grilla para calcular la distribución posterior para theta usando una grilla uniforme de 100 puntos entre 0 y 10.
- Ahora suponer que hay una nueva observación y_{n+1} = 3. Usar la distribución posterior del ítem anterior para calcular la nueva distribución posterior para theta usando la aproximación de grilla con la misma grilla que antes. 
- Discutir la secuencialidad en inferencia bayesiana.
- Con esta nueva distribución posterior para theta, encontrar la distribución posterior predictive usando simulaciones. Es decir, la distribución de observaciones que se derivan de esa distribución posterior.  

1. 

Problem 1: Prior and Posterior Predictive Checks
Consider a dataset y = (y_1, ..., y_n) where y_i ~ Binomial(5, theta) for i=1,...,n. Assume a beta prior distribution for theta with parameters a = 2 and b = 2.

(a) Using a grid approximation approach with a uniform grid of 100 points between 0 and 1, calculate the posterior distribution for theta.
(b) Generate 1000 samples from the posterior distribution of theta, and use these samples to simulate 1000 new datasets of size n = 10.
(c) Perform a prior predictive check and posterior predictive check for the original dataset y, and discuss your findings.



Problem 3: Sequential Bayesian Updating
Consider a dataset y = (y_1, ..., y_n) where y_i ~ Binomial(10, theta) for i=1,...,n. Assume a beta prior distribution for theta with parameters a = 2 and b = 2.

(a) Using a grid approximation approach with a uniform grid of 100 points between 0 and 1, calculate the posterior distribution for theta for the first 5 observations y_1, ..., y_5.
(b) Now suppose you observe a new data point y_6 = 8. Using the posterior distribution from part (a) as the prior, calculate the new posterior distribution for theta using a grid approximation approach with the same grid as before.
(c) Repeat part (b) for each subsequent observation y_7, ..., y_n. Discuss how the posterior distribution changes as more data is observed, and how this relates to the concept of sequential Bayesian updating.



  Problem 1: Grid Approximation and Sampling the Posterior
Consider a dataset y = (y_1, ..., y_n) where y_i ~ Bernoulli(theta) for i=1,...,n. Assume a beta prior distribution for theta with parameters a = 1 and b = 1.

(a) Using a grid approximation approach with a uniform grid of 100 points between 0 and 1, calculate the posterior distribution for theta.

# Set up the data and prior parameters
y <- c(1, 0, 1, 1, 0, 1, 0, 0, 1, 1)
n <- length(y)
a <- 1
b <- 1

# Set up the grid
theta_grid <- seq(0, 1, length.out = 100)

# Calculate the likelihood for each value of theta
likelihood <- ______________________________

# Calculate the unnormalized posterior
unnormalized_posterior <- ___________________________

# Normalize the posterior
posterior <- _________________________

# Plot the posterior distribution
plot(theta_grid, posterior, type = "l", xlab = "theta", ylab = "Density")


(b) Using a Metropolis-Hastings algorithm with a normal proposal distribution, sample from the posterior distribution for theta using the dataset y = (1, 0, 1, 1, 0, 1, 0, 0, 1, 1).

# Define the log-likelihood function
log_likelihood <- function(theta, y) {
  sum(dbinom(y, 1, theta, log = TRUE))
}

# Define the log-prior function
log_prior <- function(theta, a, b) {
  dbeta(theta, a, b, log = TRUE)
}

# Define the log-posterior function
log_posterior <- function(theta, y, a, b) {
  log_likelihood(theta, y) + log_prior(theta, a, b)
}

# Set the initial value for theta and the number of iterations
theta_init <- 0.5
n_iter <- 10000

# Define the standard deviation of the proposal distribution
sd_proposal <- 0.1

# Create a vector to store the samples
theta_samples <- numeric(n_iter)

# Set the first value to the initial value
theta_samples[1] <- theta_init

# Define the acceptance rate
acceptance_rate <- 0

# Run the Metropolis-Hastings algorithm
for (i in 2:n_iter) {
  # Sample from the proposal distribution
  theta_proposal <- rnorm(1, theta_samples[i-1], sd_proposal)
  
  # Calculate the acceptance probability
  log_accept_prob <- min(0, log_posterior(theta_proposal, y, a, b) - log_posterior(theta_samples[i-1], y, a, b))
  accept_prob <- exp(log_accept_prob)
  
  # Accept or reject the proposal
  if (runif(1) < accept_prob) {
    theta_samples[i] <- theta_proposal
    acceptance_rate <- acceptance_rate + 1/n_iter
  } else {
    theta_samples[i] <- theta_samples[i-1]
  }
}

# Plot the posterior distribution
hist(theta_samples, breaks = 30, freq = FALSE, xlab = "theta", main = "Posterior Distribution")





Problem 3: Posterior Predictive Checks
Consider a dataset y = (y_1, ..., y_n) where y_i ~ Poisson(lambda) for i=1,...,n. Assume a gamma(2, 1) prior distribution for lambda.

(a) Calculate the posterior predictive distribution for a new data point y_{n+1} using the dataset y = (1, 3, 0, 2, 1, 5, 2, 3, 1, 2).

# Set up the data and prior parameters
y <- c(1, 3, 0, 2, 1, 5, 2, 3, 1, 2)
n <- length(y)
a <- 2
b <- 1

# Set up the grid
lambda_grid <- seq(0, 20, length.out = 1000)

# Calculate the unnormalized posterior
unnormalized_posterior <- dgamma(lambda_grid, a + sum(y), b + n)

# Normalize the posterior
posterior <- unnormalized_posterior / sum(unnormalized_posterior)

# Calculate the posterior predictive distribution for a new data point
posterior_predictive <- dpois(y = 1, lambda = lambda_grid) * posterior

# Calculate the posterior predictive distribution for a new data point
posterior_predictive <- dpois(y = 1, lambda = lambda_grid) * posterior

# Calculate the predictive mean and variance
predictive_mean <- sum(posterior_predictive * lambda_grid)
predictive_var <- sum(posterior_predictive * (lambda_grid - predictive_mean)^2)

# Print the results
cat("Predictive mean:", predictive_mean, "\n")
cat("Predictive variance:", predictive_var, "\n")


(b) Perform a posterior predictive check by generating 1000 new datasets from the posterior predictive distribution and calculating the proportion of datasets for which the sum of the new data points is greater than or equal to the sum of the original data points.

# Generate 1000 new datasets from the posterior predictive distribution
n_sims <- 1000
new_data <- rpois(n = n_sims, lambda = rnorm(n_sims, predictive_mean, sqrt(predictive_var)))

# Calculate the sum of the new data points for each simulated dataset
new_data_sum <- apply(new_data, 2, sum)

# Calculate the proportion of datasets for which the sum of the new data points is greater than or equal to the sum of the original data points
prop_greater_or_equal <- mean(new_data_sum >= sum(y))

# Print the results
cat("Proportion of datasets for which the sum of the new data points is greater than or equal to the sum of the original data points:", prop_greater_or_equal, "\n")
